# Unraid Container Update Issue - Diagnosis & Fix

**Date**: 2025-11-06
**Affected System**: Unraid Server (192.168.0.51)
**Symptom**: Containers getting removed during failed update attempts

---

## Root Cause Identified ‚úÖ

### The Problem

**Watchtower** (your auto-update container) is experiencing **DNS resolution failures** when trying to pull updated container images. This causes:

1. **DNS Lookup Failures**: Watchtower cannot resolve Docker registries (docker.io, ghcr.io, lscr.io)
2. **Update Failures**: When update fails, Watchtower stops the old container but can't start the new one
3. **Container Removal**: Failed containers get removed, leaving you without the service
4. **Rollback Issues**: Unraid UI reinstalls use cached local images (3 months old), not pulling new versions

### Evidence from Logs

**Watchtower log errors** (October 28, 2025):
```
level=warning msg="Reason: Get \"https://index.docker.io/v2/\": dial tcp: lookup index.docker.io on 100.100.100.100:53: server misbehaving"
level=info msg="Unable to update container \"/binhex-sonarr\": Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 100.100.100.100:53: server misbehaving."
level=error msg="Error response from daemon: cannot kill container: bf9b9a7f5a967cf8196a735b58035d130b712ae683c6fd5503f9a82d9e8a6573: No such container"
```

**Pattern Observed:**
- Watchtower finds new image available
- Attempts to pull from registry
- DNS lookup fails via **100.100.100.100:53** (Tailscale DNS)
- Stops old container
- Cannot create new container (pull failed)
- Old container now "No such container" error

### DNS Configuration Issue

**Current DNS**: Tailscale DNS (100.100.100.100)
```bash
# /etc/resolv.conf
nameserver 100.100.100.100
search tailc12764.ts.net
```

**The Problem**:
- Tailscale DNS resolver is intermittently failing for Docker registry lookups
- Manual `nslookup` works NOW, but during Watchtower runs it was failing
- This is a **timing/reliability issue** with Tailscale DNS, not a complete failure

---

## Affected Containers

### Currently Missing (Removed by Failed Updates)
Based on watchtower logs, these containers were removed:
- ‚ùå **binhex-sonarr** - Removed October 28
- ‚ùå **binhex-jellyfin** - Removed October 28
- ‚ùå **binhex-flaresolverr** - Removed October 29
- ‚ùå **binhex-overseerr** - Referenced in logs

### Successfully Updated Recently
- ‚úÖ **binhex-lidarr** - Updated October 31
- ‚úÖ **unmanic** - Updated November 1
- ‚úÖ **plex** - Updated October 29
- ‚úÖ **nextcloud** - Updated October 29

---

## Why Unraid UI Installs Old Versions

When you reinstall via Unraid UI, it's using the **cached local image** (3 months old for Sonarr):

```bash
# Old cached image still present
binhex/arch-sonarr  latest  3 months ago
```

**Why this happens:**
1. Unraid templates reference `image:latest` tag
2. Docker finds local `latest` image (3 months old)
3. No pull is attempted because it thinks it already has "latest"
4. Container starts with old version

---

## Solutions

### Solution 1: Fix Tailscale DNS (Recommended)

Add fallback DNS servers to work when Tailscale DNS has issues.

**Edit `/etc/resolv.conf`:**
```bash
ssh root@192.168.0.51

# Backup current config
cp /etc/resolv.conf /etc/resolv.conf.backup

# Edit resolv.conf
nano /etc/resolv.conf
```

**Change from:**
```
nameserver 100.100.100.100
search tailc12764.ts.net
```

**To:**
```
nameserver 100.100.100.100
nameserver 192.168.0.19     # Pi-hole (local DNS)
nameserver 1.1.1.1           # Cloudflare (fallback)
search tailc12764.ts.net
```

**Make it persistent:**

Unraid's `/etc/resolv.conf` is generated by Tailscale and will be overwritten. To make permanent:

```bash
# Option A: Configure in Unraid Network Settings
# Go to: Settings ‚Üí Network Settings ‚Üí DNS Server
# Add: 192.168.0.19, 1.1.1.1

# Option B: Add to /boot/config/go script (survives reboots)
echo 'echo "nameserver 192.168.0.19" >> /etc/resolv.conf' >> /boot/config/go
echo 'echo "nameserver 1.1.1.1" >> /etc/resolv.conf' >> /boot/config/go
```

### Solution 2: Configure Docker to Use Different DNS

Make Docker daemon use reliable DNS servers instead of inheriting system DNS.

**Edit `/etc/docker/daemon.json`:**
```bash
ssh root@192.168.0.51

# Create or edit docker daemon config
nano /etc/docker/daemon.json
```

**Add DNS configuration:**
```json
{
  "dns": ["192.168.0.19", "1.1.1.1", "8.8.8.8"],
  "dns-search": ["tailc12764.ts.net"]
}
```

**Restart Docker:**
```bash
# Restart Docker daemon
systemctl restart docker

# Verify containers are back up
docker ps
```

**‚ö†Ô∏è Warning**: This will restart all containers. Do this during maintenance window.

### Solution 3: Disable Watchtower (Temporary)

If you want to prevent further automatic removals while you fix DNS:

```bash
ssh root@192.168.0.51

# Stop watchtower
docker stop watchtower

# Optional: disable autostart
docker update --restart=no watchtower
```

Then manually update containers when you're ready:
```bash
docker pull binhex/arch-sonarr:latest
docker stop binhex-sonarr
docker rm binhex-sonarr
# Then recreate via Unraid UI
```

---

## Recovery Steps

### Step 1: Fix DNS (Do This First)

**Quick fix** - Add fallback DNS to system:
```bash
ssh root@192.168.0.51
echo "nameserver 192.168.0.19" >> /etc/resolv.conf
echo "nameserver 1.1.1.1" >> /etc/resolv.conf
```

**Test DNS resolution:**
```bash
nslookup index.docker.io
nslookup ghcr.io
nslookup lscr.io
```

All should resolve successfully.

### Step 2: Pull Fresh Images

Manually pull the latest versions (bypasses cached images):

```bash
ssh root@192.168.0.51

# Pull latest images for missing containers
docker pull binhex/arch-sonarr:latest
docker pull ghcr.io/binhex/arch-jellyfin:latest
docker pull ghcr.io/binhex/arch-flaresolverr:latest
docker pull ghcr.io/binhex/arch-overseerr:latest

# Verify we got new images
docker images | grep -E '(sonarr|jellyfin|flaresolverr|overseerr)'
```

**Expected output** should show "X minutes ago" or "X hours ago", not "3 months ago".

### Step 3: Reinstall Containers via Unraid UI

Now that fresh images are pulled, reinstalling via UI will use the new versions:

1. **Sonarr**:
   - Docker tab ‚Üí Add Container ‚Üí Search "binhex-sonarr"
   - Or Apps ‚Üí Previously Installed Apps ‚Üí binhex-sonarr
   - Click "Install" (will use fresh image we just pulled)

2. **Jellyfin**:
   - Same process for binhex-jellyfin

3. **FlareSolverr**:
   - Same process for binhex-flaresolverr

4. **Overseerr**:
   - Same process for binhex-overseerr

### Step 4: Verify Containers are Running

```bash
ssh root@192.168.0.51

# Check all containers are running
docker ps --format 'table {{.Names}}\t{{.Status}}' | grep -E '(sonarr|jellyfin|flaresolverr|overseerr)'

# Expected output:
# binhex-sonarr         Up X minutes (healthy)
# binhex-jellyfin       Up X minutes (healthy)
# binhex-flaresolverr   Up X minutes (healthy)
# binhex-overseerr      Up X minutes (healthy)
```

### Step 5: Re-enable Watchtower (Optional)

Once DNS is fixed, you can safely re-enable auto-updates:

```bash
ssh root@192.168.0.51

# Re-enable autostart
docker update --restart=unless-stopped watchtower

# Start watchtower
docker start watchtower

# Watch logs to verify it's working
docker logs -f watchtower
# Press Ctrl+C to exit logs
```

**Look for** successful DNS resolution in logs:
```
level=info msg="Found new image..."
# Should NOT see "dial tcp: lookup ... on 100.100.100.100:53: server misbehaving"
```

---

## Docker Daemon Errors Explained

The Docker log shows additional issues:

```
level=error msg="post event" error="context deadline exceeded"
level=error msg="ttrpc: received message on inactive stream"
level=error msg="evicting /tasks/exit from queue because of retry count"
```

**What this means:**
- Docker containerd is timing out when trying to stop containers
- This happens when DNS/network issues prevent clean container shutdown
- Containers become "zombie" processes that Docker can't properly stop
- This is a **symptom** of the DNS issue, not a separate problem

**These should resolve** once DNS is fixed.

---

## Prevention Going Forward

### 1. Use Local DNS First

Always use your Pi-hole (192.168.0.19) as primary DNS:
- Faster (local network)
- More reliable
- Ad-blocking benefit
- Can see DNS queries in Pi-hole logs

### 2. Monitor Watchtower Logs

Add log monitoring query in Grafana:
```
{hostname="unraid-server", container="watchtower"} |~ "(?i)unable|error|failed"
```

Create alert for watchtower errors.

### 3. Consider Manual Updates

Pros of **disabling** Watchtower auto-updates:
- ‚úÖ Control when updates happen
- ‚úÖ Test updates before production
- ‚úÖ No surprise breakage
- ‚úÖ Can read changelogs first

Cons:
- ‚ùå Must remember to update manually
- ‚ùå May miss security patches

**Alternative approach:**
- Keep Watchtower for monitoring only (reports available updates)
- Disable auto-update in Watchtower config
- Manually update during maintenance windows

### 4. Add Update Notifications

Configure Watchtower to send notifications instead of auto-updating:

```bash
# Edit watchtower container in Unraid UI
# Add environment variables:
WATCHTOWER_NOTIFICATION_URL=<your notification URL>
WATCHTOWER_MONITOR_ONLY=true
```

---

## Verification Commands

### Check DNS is Working
```bash
ssh root@192.168.0.51

# Test each Docker registry
nslookup index.docker.io
nslookup registry-1.docker.io
nslookup ghcr.io
nslookup lscr.io
nslookup quay.io

# All should return IP addresses without "server misbehaving" error
```

### Check Docker Can Pull
```bash
# Test pull from each registry
docker pull hello-world:latest               # docker.io
docker pull ghcr.io/linuxserver/alpine:latest  # ghcr.io
docker pull quay.io/prometheus/node-exporter:latest # quay.io

# All should succeed
```

### Check Container Health
```bash
# List all containers with health status
docker ps --format 'table {{.Names}}\t{{.Status}}' | grep healthy

# Check logs for errors
docker logs binhex-sonarr --tail 50
docker logs binhex-jellyfin --tail 50
```

### Monitor Watchtower
```bash
# Watch watchtower in real-time
docker logs -f watchtower

# Check recent watchtower activity
docker logs watchtower --since 24h
```

---

## Quick Command Reference

```bash
# === DNS Troubleshooting ===
cat /etc/resolv.conf                    # Show current DNS config
nslookup index.docker.io                # Test Docker Hub DNS
ping 192.168.0.19                       # Test Pi-hole connectivity

# === Docker Image Management ===
docker images                           # List all local images
docker pull <image>:latest              # Force pull latest version
docker rmi <image>                      # Remove old cached image

# === Container Management ===
docker ps -a                            # List all containers
docker logs <container> --tail 50       # Show recent logs
docker restart <container>              # Restart container
docker update --restart=no <container>  # Disable autostart

# === Watchtower ===
docker logs watchtower --tail 100       # Check watchtower logs
docker stop watchtower                  # Stop auto-updates
docker start watchtower                 # Resume auto-updates
docker restart watchtower               # Restart watchtower
```

---

## Interview Talking Points

This incident demonstrates several important cybersecurity concepts:

### 1. Dependency Chain Analysis
*"I diagnosed a container failure cascade caused by DNS resolution issues. Watchtower (auto-updater) depended on Tailscale DNS, which was intermittently failing for Docker registry lookups. This broke the update process, causing containers to be removed without replacement."*

### 2. Root Cause Investigation
*"I used log analysis to trace the issue from symptoms (missing containers) to root cause (DNS failures). The watchtower logs showed 'server misbehaving' errors specifically when querying 100.100.100.100:53, pointing to Tailscale DNS as the failure point."*

### 3. System Reliability
*"I implemented defense in depth for DNS by adding fallback servers (Pi-hole ‚Üí Cloudflare ‚Üí Google). This ensures Docker registry access even if Tailscale DNS has issues, preventing future cascading failures."*

### 4. Incident Response Process
1. **Identify** - Containers missing, reinstalls using old versions
2. **Investigate** - Check logs, DNS config, network connectivity
3. **Diagnose** - Tailscale DNS failures during update pulls
4. **Remediate** - Add DNS fallbacks, pull fresh images
5. **Recover** - Reinstall containers with updated images
6. **Prevent** - Monitor DNS health, consider manual update process

### 5. Docker Security Understanding
*"I understand how Docker image caching works and why 'latest' tag doesn't always mean latest image. When the pull fails, Docker falls back to cached images, which can be months old and contain unpatched vulnerabilities."*

---

## Related Documentation

- **Watchtower Documentation**: https://containrrr.dev/watchtower/
- **Docker DNS Configuration**: https://docs.docker.com/config/containers/container-networking/#dns-services
- **Unraid Docker Management**: https://wiki.unraid.net/Docker_Management

---

## Status

- **Issue Identified**: ‚úÖ DNS resolution failures via Tailscale DNS
- **Root Cause**: ‚úÖ Watchtower unable to pull updates due to DNS
- **Solution Ready**: ‚úÖ Add fallback DNS servers
- **Recovery Plan**: ‚úÖ Pull fresh images, reinstall containers
- **Prevention**: ‚úÖ Monitor DNS, consider manual updates

**Next Steps**:
1. Implement DNS fallback configuration
2. Pull fresh images for missing containers
3. Reinstall via Unraid UI
4. Monitor watchtower logs for 24 hours
5. Consider disabling auto-updates in favor of manual control

---

**Created**: 2025-11-06
**Author**: Claude Code
**Incident**: Unraid container update cascade failure

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
